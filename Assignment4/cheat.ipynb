{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[    1,  3802,  2975,  1051,  4947,    43,   852,   201,   699,    48,\n",
      "            22,   806,    33,   254,   399,    49,    89,  1114,     2,  1051,\n",
      "          4947,     9,   254,   399,    45,   195,   201,    89,  1114,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  3802,  2975,   283,  4947,   178,    75,  1147,   450,     7,\n",
      "           218,     2,  3802,  2975,   283,  4947,  1147,   450,    40,    13,\n",
      "            10,   614,   356,     2,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   836,    75,   335,     7,    86,    89,   136,   283,  4947,\n",
      "          1915,   269,     9,   340,     9,   215,   334,     2,    86,   136,\n",
      "           283,  4947,  1915,   269,     2,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  3802,  2975,  1051,  4947,    65,    84,    36,   143,   791,\n",
      "            95,   480,  1349,     7,   218,     2,  1051,  4947,   116,   480,\n",
      "          1349,    65,   876,     2,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,    75,     5,   283,  4947,  1915,   269,    10, 17963, 17963,\n",
      "         17963,     4,   201,   699,   936,   356,    10, 17963, 17963, 17963,\n",
      "             2,    75,     5,   283,  4947,     4,   136,   215,    39,    61,\n",
      "           178,   408,    75,   201, 17963, 17963, 17963,   183,     4,    75,\n",
      "            67,   324,   559,    15,     7,    86,  1353,   541,   132,   269,\n",
      "            75,   149,   310,   201, 17963, 17963, 17963,   183,     2],\n",
      "        [    1,  3802,  2975,  1051,  4947,     5,   852,   130,    48,    22,\n",
      "           158,   938,   647,    16,     2,  3802,  2975,  1051,  4947,   377,\n",
      "          1068,   139,    48,    22,  1051,   699,  1114,     2,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   278,    50,   936,   356,    88,   124,   283,  4947,   414,\n",
      "          1046,     2,    13,   614,   356,   154,    72,    88,   124,   283,\n",
      "          4947,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,    29,   283,  4947,   936,   356,  1042,    16,    15,   320,\n",
      "          1039,     2,    75,    10,   358,    29,  3802,  2975,   283,  4947,\n",
      "             4,    13,   614,   356,  1042,    16,    15,   610,   320,  1039,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   142,   658,   222,    13,   305,   278,  1531,    29,   246,\n",
      "            83,  1051,  4947,     5,  2494,   699,   936,   356,   201,     2,\n",
      "            75,  1051,  4947,   265,   201,   334,     4,    13,   614,   356,\n",
      "           658,   222,    16,    15,   305,   278,  1531,     2,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   283,  4947,    16,   566,   453,     9,   340,     9,    17,\n",
      "           453,     2,   283,  4947,    16,    29,   113,    16,    32,    66,\n",
      "            21,   453,    29,     2,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "Segment IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Labels: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import importlib\n",
    "\n",
    "# 重新加载data模块\n",
    "importlib.reload(data)\n",
    "word_dict = data.load_vocab()\n",
    "\n",
    "# 加载数据集\n",
    "train_data, dev_data, test_data = data.load_afqmc_data(\"/Users/golden/Downloads/深度学习作业/深度学习第四次作业/src/AFQMC数据集\")\n",
    "\n",
    "# 选择一小部分数据进行处理\n",
    "sample_data = train_data[:10]  # 取训练集的前10条数据进行示例\n",
    "\n",
    "# 处理数据\n",
    "processed_sample = [data.words2id(example) for example in sample_data]\n",
    "\n",
    "# 创建小批量数据并对齐\n",
    "(input_ids, segment_ids), labels = data.collate_fn(processed_sample)\n",
    "\n",
    "# 打印验证一条mini-batch的数据\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Segment IDs:\", segment_ids)\n",
    "print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in trainloader: 1073\n",
      "Number of batches in devloader: 135\n",
      "Number of batches in testloader: 121\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "processed_train = [data.words2id(example) for example in train_data]\n",
    "processed_dev = [data.words2id(example) for example in dev_data]\n",
    "processed_test = [data.words2id(example) for example in test_data]\n",
    "\n",
    "trainloader=DataLoader(processed_train, batch_size=32, collate_fn=data.collate_fn,shuffle=True)\n",
    "devloader=DataLoader(processed_dev, batch_size=32, collate_fn=data.collate_fn,shuffle=False)\n",
    "testloader=DataLoader(processed_test, batch_size=32, collate_fn=data.collate_fn,shuffle=False)\n",
    "\n",
    "# 打印批次数量\n",
    "print(\"Number of batches in trainloader:\", len(trainloader))\n",
    "print(\"Number of batches in devloader:\", len(devloader))\n",
    "print(\"Number of batches in testloader:\", len(testloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Proportion: 0.6920545232131415\n",
      "Class 1 Proportion: 0.3079454767868585\n"
     ]
    }
   ],
   "source": [
    "def calculate_label_proportions_from_dataloader(dataloader):\n",
    "    count_class_0 = 0\n",
    "    count_class_1 = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for _, labels in dataloader:\n",
    "        count_class_0 += (labels == 0).sum().item()\n",
    "        count_class_1 += (labels == 1).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "\n",
    "    proportion_class_0 = count_class_0 / total_count\n",
    "    proportion_class_1 = count_class_1 / total_count\n",
    "\n",
    "    return proportion_class_0, proportion_class_1\n",
    "\n",
    "# 假设 trainloader 是包含您数据集的 DataLoader\n",
    "proportion_class_0, proportion_class_1 = calculate_label_proportions_from_dataloader(trainloader)\n",
    "\n",
    "print(f\"Class 0 Proportion: {proportion_class_0}\")\n",
    "print(f\"Class 1 Proportion: {proportion_class_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.segment_embeddings = nn.Embedding(2, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def _init_token_embeddings(self):\n",
    "        # 使用高斯分布初始化词嵌入的权重，标准差为 1 / sqrt(embedding_dim)\n",
    "        nn.init.normal_(self.token_embeddings.weight, mean=0, std=1 / math.sqrt(self.embedding_dim))\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.float32, device=input_ids.device).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2, dtype=torch.float32, device=input_ids.device) * -(math.log(10000.0) / self.embedding_dim))\n",
    "        div_term = div_term.unsqueeze(0)  # 形状变为(1, embedding_dim // 2)\n",
    "\n",
    "        # 创建正弦和余弦值\n",
    "        sin_values = torch.sin(position_ids * div_term)\n",
    "        cos_values = torch.cos(position_ids * div_term)\n",
    "\n",
    "    # 交替填充正弦和余弦值\n",
    "        position_embedding = torch.zeros((seq_length, self.embedding_dim), device=input_ids.device)\n",
    "        position_embedding[:, 0::2] = sin_values\n",
    "        position_embedding[:, 1::2] = cos_values[:seq_length, :self.embedding_dim // 2] if self.embedding_dim % 2 == 0 else cos_values[:seq_length, :(self.embedding_dim // 2) + 1]\n",
    "\n",
    "        # 获取token嵌入和段落嵌入\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        segment_embeds = self.segment_embeddings(segment_ids)\n",
    "\n",
    "    # 将三个嵌入相加\n",
    "        embeddings = token_embeds + segment_embeds + position_embedding\n",
    "        return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "InputEmbedding  Shape: torch.Size([32, 51, 768])\n",
      "\n",
      "InputEmbedding :\n",
      " tensor([[[-7.9398e-01,  1.9674e+00,  1.1969e-01,  ...,  4.0921e-01,\n",
      "          -1.6410e+00,  2.1352e+00],\n",
      "         [-6.3143e-01,  3.0182e+00,  1.3223e+00,  ..., -8.1392e-01,\n",
      "          -1.5787e+00,  9.6656e-01],\n",
      "         [-3.8515e-01,  6.0049e-01,  1.1348e+00,  ...,  3.4055e-01,\n",
      "          -7.1400e-01,  2.2651e+00],\n",
      "         ...,\n",
      "         [-3.0110e+00,  1.8789e+00,  6.1446e-01,  ...,  3.0746e-01,\n",
      "          -1.9792e+00,  7.8179e-01],\n",
      "         [-3.1965e+00,  2.8197e+00, -2.9954e-01,  ...,  3.0746e-01,\n",
      "          -1.9791e+00,  7.8179e-01],\n",
      "         [-2.5051e+00,  3.4840e+00, -6.3683e-01,  ...,  3.0746e-01,\n",
      "          -1.9790e+00,  7.8179e-01]],\n",
      "\n",
      "        [[-7.9398e-01,  1.9674e+00,  1.1969e-01,  ...,  4.0921e-01,\n",
      "          -1.6410e+00,  2.1352e+00],\n",
      "         [-6.6358e-01,  1.4178e+00,  9.8791e-01,  ...,  8.8682e-01,\n",
      "          -1.8639e+00, -5.3761e-02],\n",
      "         [-8.5917e-01,  3.4996e+00,  3.6076e+00,  ...,  2.5524e-01,\n",
      "          -2.2028e+00,  7.2234e-01],\n",
      "         ...,\n",
      "         [-3.0110e+00,  1.8789e+00,  6.1446e-01,  ...,  3.0746e-01,\n",
      "          -1.9792e+00,  7.8179e-01],\n",
      "         [-3.1965e+00,  2.8197e+00, -2.9954e-01,  ...,  3.0746e-01,\n",
      "          -1.9791e+00,  7.8179e-01],\n",
      "         [-2.5051e+00,  3.4840e+00, -6.3683e-01,  ...,  3.0746e-01,\n",
      "          -1.9790e+00,  7.8179e-01]],\n",
      "\n",
      "        [[-7.9398e-01,  1.9674e+00,  1.1969e-01,  ...,  4.0921e-01,\n",
      "          -1.6410e+00,  2.1352e+00],\n",
      "         [-1.0260e+00,  2.5038e+00, -2.6756e-01,  ..., -7.6378e-01,\n",
      "          -1.8108e-01,  1.2130e+00],\n",
      "         [-1.4936e+00,  1.8773e+00,  9.3769e-01,  ...,  1.3860e+00,\n",
      "          -3.3047e-01,  1.2602e+00],\n",
      "         ...,\n",
      "         [-3.0110e+00,  1.8789e+00,  6.1446e-01,  ...,  3.0746e-01,\n",
      "          -1.9792e+00,  7.8179e-01],\n",
      "         [-3.1965e+00,  2.8197e+00, -2.9954e-01,  ...,  3.0746e-01,\n",
      "          -1.9791e+00,  7.8179e-01],\n",
      "         [-2.5051e+00,  3.4840e+00, -6.3683e-01,  ...,  3.0746e-01,\n",
      "          -1.9790e+00,  7.8179e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.9398e-01,  1.9674e+00,  1.1969e-01,  ...,  4.0921e-01,\n",
      "          -1.6410e+00,  2.1352e+00],\n",
      "         [ 5.7057e-01,  3.5556e+00,  2.2154e+00,  ...,  1.1845e+00,\n",
      "           9.3408e-01,  1.4751e-01],\n",
      "         [ 3.1628e-01,  2.1802e+00, -8.0961e-04,  ...,  5.4341e-01,\n",
      "          -2.3851e+00,  6.2436e-01],\n",
      "         ...,\n",
      "         [-3.0110e+00,  1.8789e+00,  6.1446e-01,  ...,  3.0746e-01,\n",
      "          -1.9792e+00,  7.8179e-01],\n",
      "         [-3.1965e+00,  2.8197e+00, -2.9954e-01,  ...,  3.0746e-01,\n",
      "          -1.9791e+00,  7.8179e-01],\n",
      "         [-2.5051e+00,  3.4840e+00, -6.3683e-01,  ...,  3.0746e-01,\n",
      "          -1.9790e+00,  7.8179e-01]],\n",
      "\n",
      "        [[-7.9398e-01,  1.9674e+00,  1.1969e-01,  ...,  4.0921e-01,\n",
      "          -1.6410e+00,  2.1352e+00],\n",
      "         [-1.0260e+00,  2.5038e+00, -2.6756e-01,  ..., -7.6378e-01,\n",
      "          -1.8108e-01,  1.2130e+00],\n",
      "         [-1.4936e+00,  1.8773e+00,  9.3769e-01,  ...,  1.3860e+00,\n",
      "          -3.3047e-01,  1.2602e+00],\n",
      "         ...,\n",
      "         [-3.0110e+00,  1.8789e+00,  6.1446e-01,  ...,  3.0746e-01,\n",
      "          -1.9792e+00,  7.8179e-01],\n",
      "         [-3.1965e+00,  2.8197e+00, -2.9954e-01,  ...,  3.0746e-01,\n",
      "          -1.9791e+00,  7.8179e-01],\n",
      "         [-2.5051e+00,  3.4840e+00, -6.3683e-01,  ...,  3.0746e-01,\n",
      "          -1.9790e+00,  7.8179e-01]],\n",
      "\n",
      "        [[-7.9398e-01,  1.9674e+00,  1.1969e-01,  ...,  4.0921e-01,\n",
      "          -1.6410e+00,  2.1352e+00],\n",
      "         [ 1.9474e-01,  3.2945e+00, -8.0193e-01,  ...,  5.8606e-01,\n",
      "          -2.4705e+00,  8.8771e-02],\n",
      "         [-7.8118e-01,  1.9437e+00,  1.0067e+00,  ..., -6.6964e-01,\n",
      "          -1.2101e+00, -3.2448e-01],\n",
      "         ...,\n",
      "         [-3.0110e+00,  1.8789e+00,  6.1446e-01,  ...,  3.0746e-01,\n",
      "          -1.9792e+00,  7.8179e-01],\n",
      "         [-3.1965e+00,  2.8197e+00, -2.9954e-01,  ...,  3.0746e-01,\n",
      "          -1.9791e+00,  7.8179e-01],\n",
      "         [-2.5051e+00,  3.4840e+00, -6.3683e-01,  ...,  3.0746e-01,\n",
      "          -1.9790e+00,  7.8179e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 768\n",
    "max_seq_len = 512\n",
    "vocab_size = len(word_dict)\n",
    "embedding_layer=Embeddings(vocab_size,embedding_dim,max_seq_len)\n",
    "InputEmbedding = embedding_layer(input_ids, segment_ids)\n",
    "#print(\"Input IDs:\\n\", input_ids)\n",
    "#print(\"\\nSegment IDs:\\n\", segment_ids)\n",
    "print(\"\\nInputEmbedding  Shape:\", InputEmbedding.shape)\n",
    "print(\"\\nInputEmbedding :\\n\", InputEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    \"\"\"计算缩放点积注意力\"\"\"\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "    # 缩放\n",
    "    depth = query.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(depth)\n",
    "\n",
    "    # softmax应用于最后一个轴\n",
    "    attention_weights = nn.functional.softmax(scaled_attention_logits, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 确保embedding_dim可以被num_heads整除以便均匀分割到每个头\n",
    "        assert embedding_dim % num_heads == 0\n",
    "\n",
    "        self.depth = embedding_dim // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.wk = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.wv = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.dense = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def split_into_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        q = self.split_into_heads(self.wq(x), batch_size)  # (batch_size, num_heads, seq_length, depth)\n",
    "        k = self.split_into_heads(self.wk(x), batch_size)\n",
    "        v = self.split_into_heads(self.wv(x), batch_size)\n",
    "\n",
    "        # 缩放点积注意力\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)  # (batch_size, seq_length, num_heads, depth)\n",
    "\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.embedding_dim)\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output,attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        # sublayer_output是子层（例如多头自注意力层）的输出\n",
    "        dropout_output = self.dropout(sublayer_output)\n",
    "        return self.norm(x + dropout_output)  # 残差连接和层归一化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ff_dim, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embedding_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, attention_weights = self.multi_head_attention(x)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm2(x + self.dropout(ffn_output))\n",
    "        return x,attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticMatchingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, ff_dim, max_seq_len, num_encoder_layers=2, dropout=0):\n",
    "        super(SemanticMatchingModel, self).__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, embedding_dim, max_seq_len)\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(embedding_dim, num_heads, ff_dim, dropout) for _ in range(num_encoder_layers)])\n",
    "        self.classifier = nn.Linear(embedding_dim, 2)  # 假设输出是二分类\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        x = self.embeddings(input_ids, segment_ids)\n",
    "        all_attention_weights = []\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x, attention_weights = encoder_layer(x)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "        logits = self.classifier(x[:, 0])\n",
    "        # 取编码器输出的第一个元素（对应[CLS]标记）用于分类\n",
    "        return logits,all_attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_warmup(current_step, warmup_steps, base_lr, max_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return base_lr + (max_lr - base_lr) * current_step / warmup_steps\n",
    "    else:\n",
    "        return max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticMatchingModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (token_embeddings): Embedding(17964, 768)\n",
       "    (segment_embeddings): Embedding(2, 768)\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x TransformerEncoderLayer(\n",
       "      (multi_head_attention): MultiHeadSelfAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "model = SemanticMatchingModel(vocab_size = len(word_dict),embedding_dim=embedding_dim,\n",
    "                              num_heads=16,ff_dim=4096,max_seq_len=max_seq_len)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticMatchingModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (token_embeddings): Embedding(17964, 768)\n",
      "    (segment_embeddings): Embedding(2, 768)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerEncoderLayer(\n",
      "      (multi_head_attention): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=4096, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=4096, out_features=768, bias=True)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94c3a772f6a422fa5f1422309df3dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceeafc9574354219a66c100aa5010eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36e9b52911a4dbd8cf0ec878834543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [302]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m outputs,all_attention_weights \u001b[38;5;241m=\u001b[39m model(input_ids, segment_ids)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# 初始化模型、损失函数、优化器\n",
    "model = SemanticMatchingModel(vocab_size = len(word_dict),embedding_dim=embedding_dim,\n",
    "                              num_heads=16,ff_dim=4096,max_seq_len=max_seq_len)\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "print(model)\n",
    "weight_for_class_0 = 1 / proportion_class_0\n",
    "weight_for_class_1 = 1 / proportion_class_1\n",
    "class_weights = torch.tensor([weight_for_class_0, weight_for_class_1]) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "warmup_steps = 250\n",
    "num_epochs=10\n",
    "total_steps = num_epochs * len(trainloader)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_warmup(step, warmup_steps, 1e-6, 1e-4))\n",
    "\n",
    "# 初始化TensorBoard\n",
    "writer = SummaryWriter('logs/semantic_matching')\n",
    "best_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc='Training')\n",
    "    for i, batch in progress_bar:\n",
    "        (input_ids, segment_ids), labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs,all_attention_weights = model(input_ids, segment_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=np.mean(train_losses), accuracy=100. * train_correct / train_total)\n",
    "\n",
    "    # 验证循环\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    progress_bar = tqdm(enumerate(devloader), total=len(devloader), desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in progress_bar:\n",
    "            (input_ids, segment_ids), labels = batch\n",
    "            outputs,all_attention_weights = model(input_ids, segment_ids)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=np.mean(val_losses), accuracy=100. * val_correct / val_total)\n",
    "\n",
    "        accuracy = 100. * val_correct / val_total\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    # 记录到TensorBoard\n",
    "    writer.add_scalar('Training Loss', np.mean(train_losses), epoch)\n",
    "    writer.add_scalar('Training Accuracy', 100. * train_correct / train_total, epoch)\n",
    "    writer.add_scalar('Validation Loss', np.mean(val_losses), epoch)\n",
    "    writer.add_scalar('Validation Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch * len(trainloader) + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 删除旧的TensorBoard日志目录（谨慎操作，以免丢失数据）\n",
    "log_dir = 'logs/semantic_matching'\n",
    "try:\n",
    "    shutil.rmtree(log_dir)\n",
    "except OSError as e:\n",
    "    print(f\"Error: {log_dir} : {e.strerror}\")\n",
    "\n",
    "# 创建新的TensorBoard写入器\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# 创建新的模型实例\n",
    "model = SemanticMatchingModel(vocab_size = len(word_dict),embedding_dim=embedding_dim,\n",
    "                              num_heads=8,ff_dim=2048,max_seq_len=max_seq_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 创建新的优化器\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the 50 random test samples: 68.0%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 加载保存的模型\n",
    "model = SemanticMatchingModel(vocab_size = len(word_dict),embedding_dim=embedding_dim,\n",
    "                              num_heads=16,ff_dim=4096,max_seq_len=max_seq_len)  # 用与训练时相同的参数初始化模型\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "# 从测试集中随机选择50条数据\n",
    "random_samples = random.sample(processed_dev, 50)\n",
    "\n",
    "# 创建一个DataLoader来加载这些样本\n",
    "sample_loader = DataLoader(random_samples, batch_size=len(random_samples), collate_fn=data.collate_fn)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in sample_loader:\n",
    "        (input_ids, segment_ids), labels = batch\n",
    "        outputs,all_attention_weights = model(input_ids, segment_ids)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the 50 random test samples: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 20511 (\\N{CJK UNIFIED IDEOGRAPH-501F}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 21591 (\\N{CJK UNIFIED IDEOGRAPH-5457}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 36824 (\\N{CJK UNIFIED IDEOGRAPH-8FD8}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 28165 (\\N{CJK UNIFIED IDEOGRAPH-6E05}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 21518 (\\N{CJK UNIFIED IDEOGRAPH-540E}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 22810 (\\N{CJK UNIFIED IDEOGRAPH-591A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 20037 (\\N{CJK UNIFIED IDEOGRAPH-4E45}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 33021 (\\N{CJK UNIFIED IDEOGRAPH-80FD}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 29992 (\\N{CJK UNIFIED IDEOGRAPH-7528}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 39057 (\\N{CJK UNIFIED IDEOGRAPH-9891}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 32321 (\\N{CJK UNIFIED IDEOGRAPH-7E41}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 20351 (\\N{CJK UNIFIED IDEOGRAPH-4F7F}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/utils.py:95: UserWarning: Glyph 27454 (\\N{CJK UNIFIED IDEOGRAPH-6B3E}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20511 (\\N{CJK UNIFIED IDEOGRAPH-501F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 21591 (\\N{CJK UNIFIED IDEOGRAPH-5457}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 36824 (\\N{CJK UNIFIED IDEOGRAPH-8FD8}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 28165 (\\N{CJK UNIFIED IDEOGRAPH-6E05}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 21518 (\\N{CJK UNIFIED IDEOGRAPH-540E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 22810 (\\N{CJK UNIFIED IDEOGRAPH-591A}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20037 (\\N{CJK UNIFIED IDEOGRAPH-4E45}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 33021 (\\N{CJK UNIFIED IDEOGRAPH-80FD}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 29992 (\\N{CJK UNIFIED IDEOGRAPH-7528}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 39057 (\\N{CJK UNIFIED IDEOGRAPH-9891}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 32321 (\\N{CJK UNIFIED IDEOGRAPH-7E41}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20351 (\\N{CJK UNIFIED IDEOGRAPH-4F7F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 27454 (\\N{CJK UNIFIED IDEOGRAPH-6B3E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '借', '呗', '还', '清', '后', '多', '久', '还', '能', '用', '[SEP]', '频', '繁', '使', '用', '借', '呗', '还', '款', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "sample_id = 30 # 选择样本的索引\n",
    "sample_input_ids = input_ids[sample_id].unsqueeze(0)\n",
    "sample_segment_ids = segment_ids[sample_id].unsqueeze(0)  # 如果模型使用了segment_ids\n",
    "\n",
    "index_to_word = {idx: word for word, idx in word_dict.items()}\n",
    "input_tokens = [index_to_word.get(index.item(), '[UNK]') for index in sample_input_ids[0]]\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 通过模型传递数据\n",
    "    output, all_attention_weights = model(sample_input_ids, sample_segment_ids)\n",
    "# 假设您想查看第一层第一个头的注意力权重的形状\n",
    "layer = 0\n",
    "head = 15\n",
    "\n",
    "\n",
    "attention_weights_layer= all_attention_weights[layer].squeeze(0)  # 移除批次维度，如果有的话\n",
    "attention_weights_head=attention_weights_layer[head].squeeze(0)\n",
    "print(attention_weights_head.shape)\n",
    "# 将权重转换为numpy数组以便可视化\n",
    "attention_weights_numpy = attention_weights_head.detach().numpy()\n",
    "\n",
    "pad_token_id = word_dict['[PAD]']\n",
    "\n",
    "# 找到第一个填充项的索引\n",
    "pad_index = (sample_input_ids[0] == pad_token_id).nonzero(as_tuple=True)[0]\n",
    "if len(pad_index) > 0:\n",
    "    pad_index = pad_index[0].item()  # 第一个填充项的索引\n",
    "else:\n",
    "    pad_index = sample_input_ids[0].size(0)  # 没有填充项，使用整个长度\n",
    "\n",
    "# 切片以排除填充项\n",
    "attention_weights_sliced = attention_weights_numpy[:pad_index, :pad_index]\n",
    "input_tokens_sliced = input_tokens[:pad_index]\n",
    "\n",
    "# 创建热力图\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights_sliced, xticklabels=input_tokens_sliced, yticklabels=input_tokens_sliced, cmap='viridis', annot=False)\n",
    "\n",
    "# 设置标题和坐标轴标签\n",
    "plt.title(f'Attention Weights Layer {layer+1} Head {head+1}')\n",
    "plt.xlabel('Key Tokens')\n",
    "plt.ylabel('Query Tokens')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "print(input_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticMatchingModel2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, ff_dim, max_seq_len, num_encoder_layers=3, dropout=0.1):\n",
    "        super(SemanticMatchingModel2, self).__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, embedding_dim, max_seq_len)\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(embedding_dim, num_heads, ff_dim, dropout) for _ in range(num_encoder_layers)])\n",
    "        self.classifier = nn.Linear(embedding_dim, 2)  # 假设输出是二分类\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        x = self.embeddings(input_ids, segment_ids)\n",
    "        all_attention_weights = []\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x, attention_weights = encoder_layer(x)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "        logits = self.classifier(x[:, 0])\n",
    "        # 取编码器输出的第一个元素（对应[CLS]标记）用于分类\n",
    "        return logits,all_attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticMatchingModel2(\n",
      "  (embeddings): Embeddings(\n",
      "    (token_embeddings): Embedding(17964, 768)\n",
      "    (segment_embeddings): Embedding(2, 768)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-3): 4 x TransformerEncoderLayer(\n",
      "      (multi_head_attention): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=4096, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=4096, out_features=768, bias=True)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce5166dff6d4c56945a458d62b8cc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776f874fd8834cf29e05de1f80552d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b96dd0b08041aba4360723a5f86796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64e8b0636524a71a2f74c7b6877462b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7036f91024844d87bbd5af2e042b00e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa3a16c25fb4e73aa5234c94eaf3170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e2312638364e00ade8dda4efef4c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2f87e301af457f9422f8c59be85f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04cb3d9687e45b5964586a2b286a8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18213275de242179031ccdd9cc2aed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepermodel = SemanticMatchingModel2(vocab_size = len(word_dict),embedding_dim=embedding_dim,\n",
    "                              num_heads=16,ff_dim=4096,max_seq_len=max_seq_len)\n",
    "\n",
    "print(deepermodel)\n",
    "deepermodel.apply(weights_init)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(deepermodel.parameters(), lr=1e-4)\n",
    "warmup_steps = 500\n",
    "num_epochs=5\n",
    "total_steps = num_epochs * len(trainloader)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_warmup(step, warmup_steps, 1e-6, 1e-4))\n",
    "\n",
    "# 初始化TensorBoard\n",
    "writer = SummaryWriter('logs/semantic_matching_deeper')\n",
    "best_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    deepermodel.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc='Training')\n",
    "    for i, batch in progress_bar:\n",
    "        (input_ids, segment_ids), labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs,all_attention_weights = deepermodel(input_ids, segment_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=np.mean(train_losses), accuracy=100. * train_correct / train_total)\n",
    "\n",
    "    # 验证循环\n",
    "    deepermodel.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    progress_bar = tqdm(enumerate(devloader), total=len(devloader), desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in progress_bar:\n",
    "            (input_ids, segment_ids), labels = batch\n",
    "            outputs,all_attention_weights = deepermodel(input_ids, segment_ids)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=np.mean(val_losses), accuracy=100. * val_correct / val_total)\n",
    "\n",
    "        accuracy = 100. * val_correct / val_total\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model_deeper.pth')\n",
    "\n",
    "    # 记录到TensorBoard\n",
    "    writer.add_scalar('Training Loss', np.mean(train_losses), epoch)\n",
    "    writer.add_scalar('Training Accuracy', 100. * train_correct / train_total, epoch)\n",
    "    writer.add_scalar('Validation Loss', np.mean(val_losses), epoch)\n",
    "    writer.add_scalar('Validation Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch * len(trainloader) + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "print(model)\n",
    "weight_for_class_0 = 1 / proportion_class_0\n",
    "weight_for_class_1 = 1 / proportion_class_1\n",
    "class_weights = torch.tensor([weight_for_class_0, weight_for_class_1]) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "warmup_steps = 250\n",
    "num_epochs=10\n",
    "total_steps = num_epochs * len(trainloader)\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_warmup(step, warmup_steps, 1e-6, 1e-4))\n",
    "\n",
    "# 初始化TensorBoard\n",
    "writer = SummaryWriter('logs/bertsemantic_matching')\n",
    "best_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc='Training')\n",
    "    for i, batch in progress_bar:\n",
    "        (input_ids, segment_ids), labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs,all_attention_weights = model(input_ids, segment_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=np.mean(train_losses), accuracy=100. * train_correct / train_total)\n",
    "\n",
    "    # 验证循环\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    progress_bar = tqdm(enumerate(devloader), total=len(devloader), desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in progress_bar:\n",
    "            (input_ids, segment_ids), labels = batch\n",
    "            outputs,all_attention_weights = model(input_ids, segment_ids)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=np.mean(val_losses), accuracy=100. * val_correct / val_total)\n",
    "\n",
    "        accuracy = 100. * val_correct / val_total\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    # 记录到TensorBoard\n",
    "    writer.add_scalar('Training Loss', np.mean(train_losses), epoch)\n",
    "    writer.add_scalar('Training Accuracy', 100. * train_correct / train_total, epoch)\n",
    "    writer.add_scalar('Validation Loss', np.mean(val_losses), epoch)\n",
    "    writer.add_scalar('Validation Accuracy', accuracy, epoch)\n",
    "    #writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch * len(trainloader) + i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
