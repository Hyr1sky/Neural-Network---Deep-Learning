{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 作业四 AFQMC </center>\n",
    "by Hyr1sky_He\n",
    "\n",
    "_为了提高任务效率，本ipynb中不再细化每个步骤的操作过程，仅记录重要思路及核心方法，Task题目解答将在实验报告中给出_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import jieba\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Assignment4_dataset/data/vocab.txt\n",
      "../Assignment4_dataset/data/AFQMC数据集/test.json\n",
      "../Assignment4_dataset/data/AFQMC数据集/dev.json\n",
      "../Assignment4_dataset/data/AFQMC数据集/train.json\n"
     ]
    }
   ],
   "source": [
    "# read json & vocab\n",
    "def check_data(path):\n",
    "    for dir_name, _, file_names in os.walk(path):\n",
    "        for file_name in file_names:\n",
    "            print(os.path.join(dir_name, file_name))\n",
    "\n",
    "check_data('../Assignment4_dataset/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "def read_data(path):\n",
    "    sentence_1 = []\n",
    "    sentence_2 = []\n",
    "    label = []\n",
    "    with open (path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = json.loads(line)\n",
    "            sentence_1.append(line['sentence1'])\n",
    "            sentence_2.append(line['sentence2'])\n",
    "            label.append(line['label'])\n",
    "        df = pd.DataFrame({'sentence1': sentence_1, 'sentence2': sentence_2, 'label': label})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
       "      <td>借呗得评估多久</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我的花呗账单是***，还款怎么是***</td>\n",
       "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentence1                              sentence2 label\n",
       "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗     0\n",
       "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么     0\n",
       "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单     0\n",
       "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久     0\n",
       "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_df(path, type):\n",
    "    df = read_data(path)\n",
    "    df.to_csv('../Assignment4_dataset/data/AFQMC数据集/' + type + '.csv', index = False)\n",
    "    return \"Generate csv file successfully!\"\n",
    "\n",
    "train_df = read_data('../Assignment4_dataset/data/AFQMC数据集/train.json')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate csv file successfully!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_df('../Assignment4_dataset/data/AFQMC数据集/train.json', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>双十一花呗提额在哪</td>\n",
       "      <td>里可以提花呗额度</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>花呗支持高铁票支付吗</td>\n",
       "      <td>为什么友付宝不支持花呗付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我的蚂蚁花呗支付金额怎么会有限制</td>\n",
       "      <td>我到支付宝实体店消费用花呗支付受金额限制</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>为什么有花呗额度不能分期付款</td>\n",
       "      <td>花呗分期额度不足</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>赠品不能设置用花呗付款</td>\n",
       "      <td>怎么不能花呗分期付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentence1             sentence2 label\n",
       "0         双十一花呗提额在哪              里可以提花呗额度     0\n",
       "1        花呗支持高铁票支付吗         为什么友付宝不支持花呗付款     0\n",
       "2  我的蚂蚁花呗支付金额怎么会有限制  我到支付宝实体店消费用花呗支付受金额限制     1\n",
       "3    为什么有花呗额度不能分期付款              花呗分期额度不足     0\n",
       "4       赠品不能设置用花呗付款            怎么不能花呗分期付款     0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = read_data('../Assignment4_dataset/data/AFQMC数据集/dev.json')\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate csv file successfully!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_df('../Assignment4_dataset/data/AFQMC数据集/dev.json', 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    34334.000000\n",
       "mean        26.732597\n",
       "std         10.405410\n",
       "min         10.000000\n",
       "25%         20.000000\n",
       "50%         25.000000\n",
       "75%         30.000000\n",
       "max        157.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df.sentence1.str.len() + train_df.sentence2.str.len()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3802, 2975, 1051, 4947, 43, 852, 201, 699, 48, 22, 806, 33, 254, 399, 49, 89, 1114]\n",
      "[1051, 4947, 9, 254, 399, 45, 195, 201, 89, 1114]\n",
      "[1, 3802, 2975, 1051, 4947, 43, 852, 201, 699, 48, 22, 806, 33, 254, 399, 49, 89, 1114, 2, 1051, 4947, 9, 254, 399, 45, 195, 201, 89, 1114, 2]\n"
     ]
    }
   ],
   "source": [
    "with open('../Assignment4_dataset/data/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f]\n",
    "\n",
    "# mapping to id\n",
    "char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "# tokenize_sample\n",
    "sentence_11 = \"蚂蚁借呗等额还款可以换成先息后本吗\"\n",
    "sentence_22 = \"借呗有先息到期还本吗\"\n",
    "sentence11_ids = [char_to_id.get(char, char_to_id['[MASK]']) for char in sentence_11]\n",
    "print(sentence11_ids)\n",
    "sentence22_ids = [char_to_id.get(char, char_to_id['[MASK]']) for char in sentence_22]\n",
    "print(sentence22_ids)\n",
    "sentence_ids = [char_to_id['[CLS]']] + sentence11_ids + [char_to_id['[SEP]']] + sentence22_ids + [char_to_id['[SEP]']]\n",
    "print(sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "num_hiddens = 256\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "max_len = 160\n",
    "num_epochs = 20\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1, 3802, 2975, 1051, 4947,   43,  852,  201,  699,   48,   22,  806,\n",
      "          33,  254,  399,   49,   89, 1114,    2, 1051, 4947,    9,  254,  399,\n",
      "          45,  195,  201,   89, 1114,    2,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, char_to_id, max_length=160):\n",
    "        self.data = dataframe\n",
    "        self.char_to_id = char_to_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence1 = self.data.iloc[index]['sentence1']\n",
    "        sentence2 = self.data.iloc[index]['sentence2']\n",
    "        label = self.data.iloc[index]['label']\n",
    "\n",
    "        combined_tokens = ['[CLS]'] + [char for char in sentence1] + ['[SEP]'] + [char for char in sentence2] + ['[SEP]']\n",
    "        segment_ids = [0] * (len(sentence1) + 2) + [1] * (len(sentence2) + 1)\n",
    "\n",
    "        combined_ids = [self.char_to_id.get(char, self.char_to_id['[MASK]']) for char in combined_tokens]\n",
    "        combined_ids = torch.nn.functional.pad(torch.tensor(combined_ids), (0, self.max_length - len(combined_ids)))\n",
    "        segment_ids = torch.nn.functional.pad(torch.tensor(segment_ids), (0, self.max_length - len(segment_ids)))\n",
    "\n",
    "        return combined_ids, segment_ids, label\n",
    "\n",
    "train_dataset = TextDataset(train_df, char_to_id)\n",
    "dev_dataset = TextDataset(dev_df, char_to_id)\n",
    "\n",
    "for combined_ids, segment_ids, label in train_dataset:\n",
    "    print(combined_ids)\n",
    "    print(segment_ids)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass TextDataset(Dataset):\\n    def __init__(self, dataframe, char_to_id， max_len = 160):\\n        self.data = dataframe\\n        self.char_to_id = char_to_id\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, index):\\n        sentence1 = self.data.iloc[index]['sentence1']\\n        sentence2 = self.data.iloc[index]['sentence2']\\n        label = self.data.iloc[index]['label']\\n\\n        combined_tokens = ['[CLS]'] + [char for char in sentence1] + ['[SEP]'] + [char for char in sentence2] + ['[SEP]']\\n        segment_ids = [0] * (len(sentence1) + 2) + [1] * (len(sentence2) + 1)\\n        combined_ids = [self.char_to_id.get(char, self.char_to_id['[MASK]']) for char in combined_tokens]\\n\\n        return combined_ids, segment_ids, label\\n\\ndef custom_collate_fn(batch):\\n    combined_ids, segment_ids, label = zip(*batch)\\n    return combined_ids, segment_ids, label\\n\\ntrain_dataset = TextDataset(train_df, char_to_id)\\ndev_dataset = TextDataset(dev_df, char_to_id)\\n\\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\\ndev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\\n\\nfor combined_ids, segment_ids, label in train_dataloader:\\n    print(combined_ids)\\n    print(segment_ids)\\n    print(label)\\n    break\\n\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, char_to_id, max_len = 160):\n",
    "        self.data = dataframe\n",
    "        self.char_to_id = char_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence1 = self.data.iloc[index]['sentence1']\n",
    "        sentence2 = self.data.iloc[index]['sentence2']\n",
    "        label = self.data.iloc[index]['label']\n",
    "\n",
    "        combined_tokens = ['[CLS]'] + [char for char in sentence1] + ['[SEP]'] + [char for char in sentence2] + ['[SEP]']\n",
    "        segment_ids = [0] * (len(sentence1) + 2) + [1] * (len(sentence2) + 1)\n",
    "        combined_ids = [self.char_to_id.get(char, self.char_to_id['[MASK]']) for char in combined_tokens]\n",
    "\n",
    "        return combined_ids, segment_ids, label\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    combined_ids, segment_ids, label = zip(*batch)\n",
    "    return combined_ids, segment_ids, label\n",
    "\n",
    "train_dataset = TextDataset(train_df, char_to_id)\n",
    "dev_dataset = TextDataset(dev_df, char_to_id)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "\n",
    "for combined_ids, segment_ids, label in train_dataloader:\n",
    "    print(combined_ids)\n",
    "    print(segment_ids)\n",
    "    print(label)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=160):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create a long P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30) must match the size of tensor b (160) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m PE \u001b[38;5;241m=\u001b[39m PositionalEncoding(num_hiddens, dropout, max_len)\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m embeddings(torch\u001b[38;5;241m.\u001b[39mtensor(sentence_ids))\n\u001b[0;32m----> 4\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, Y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 15\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 15\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(X)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (30) must match the size of tensor b (160) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "embeddings = nn.Embedding(len(vocab), num_hiddens)\n",
    "PE = PositionalEncoding(num_hiddens, dropout, max_len)\n",
    "X = embeddings(torch.tensor(sentence_ids))\n",
    "Y = PE(X)\n",
    "print(X.shape, Y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
