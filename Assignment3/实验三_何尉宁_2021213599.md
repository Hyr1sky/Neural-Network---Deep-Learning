# 《神经网络与深度学习》课程实验作业（三）
# 实验内容：深度自编码器
---
何尉宁 2021213599

## 实验一： 基于MNIST的AE
### 实验要求：

- 完成数据读写并试着搭建深度自编码器网络 
- 选择二元交叉熵函数作为损失函数，在限制bottleneck层维度为2的情况下训练模型
- 设置噪声因子为0.4，在输入图像上叠加均值为0且方差为1的标准高斯白噪声，训练降噪自编码器，并进行降噪结果展示  
- 试在问题(2)的基础上，对latent code进行均匀采样，并利用解码器对采样结果进行恢复，观察并描述所得到的结果
- 试在问题(4)的基础上，在训练深度自编码器时使用 L2正则化，观察并描述你所得到的结果。

---
### 教训：
相信Andrew Ng的话，"When you try something new, do the dirty and quick staff first"
早从MLP开始写不就好了。

---
### 材料：
1. [Anomaly Detection Using PyTorch Autoencoder and MNIST | by Benjamin | Medium](https://medium.com/@benjoe/anomaly-detection-using-pytorch-autoencoder-and-mnist-31c5c2186329)
2. [什么是深度生成模型(Deep Generative Model)?)](https://www.zhihu.com/question/310388816/answer/2801712061)

---
### 1. 数据读写与AE网络
#### 1.1 数据读写

`pytorch`里有MNIST，直接正常下载就好了

#### 1.2 网络

前后写了三版，因为最开始的CNN不尽人意，修改了很久之后还是效果很差，就换成MLP从简单开始，结果反而有更好的收效。
最后一版网络的层数最少，并且参考了李宏毅老师说的维度先增再减，达到了一个比较不错的效果。

---
### 2. 正式训练(BCE/Bottleneck = 2)

**BCE**可以看作是把图像生成作为一个二分类问题，0/1分别代表这个像素点是否需要打点。
_Loss function for binay inputs:_
$l(f(\mathbf{x}))=-\sum_k\left(x_k\log(\widehat{x}_k)+(1-x_k)\log(1-\widehat{x}_k)\right)$

恢复后的图像：
![[generative_images_9.png]]

---
### 3. Denoising-AE 加噪后训练

加入高斯噪声：
```python
	# add noise
    # x = self.add_noise(x)
    noise_factor = 0.4
    x = x + torch.normal(mean=0.0, std=1, size=x.size()) * noise_factor
```
对比图：
![[great.png]]

加入噪声后对损失函数的选择需要慎重，容易出现混杂的现象。
**对于恢复图像边界不够清晰的问题，可以尝试加入边界特征提取的`filter`对边界进行锐化，使得数字更加清晰**

---
### 4. 均匀采样后恢复

定义了`get_latent_code`和`generate_img`函数，对隐藏层进行提取，这里的采样后恢复图像的代码还可以进一步封装，但是当时为了更快出结果就没修改了。

```python
    def get_latent_code(self, x):
        # Encoder
        x1 = self.conv1(x)
        x2 = self.conv2(x1)
        x3 = self.conv3(x2)
        x4 = self.conv4(x3)
        x4 = x4.view(x4.size(0), -1)
        code = self.fc1(x4)
        return code

    def generate_img(self, latent_code):
        with torch.no_grad():
            # Decoder
            y1 = self.fc2(latent_code)
            y1 = y1.view(y1.size(0), 24, 2, 2)
            y2 = self.conv5(y1)
            y3 = self.conv6(y2)
            y4 = self.conv7(y3)
            generated_image = self.conv8(y4)
        return generated_image
```

打印latent_code分布：
各类别的分布较为分散，效果已经比较`sparse`了，后续的正则化或许只能稍微优化一下性能。
![[MSE_MSEL2.png]]

从(-5, -5)到(5, 5)均匀采样恢复数字，能看到在分界的地方会有采空的现象出现，有一些无意义的数字生成，但整体来说效果已经非常不错了，因为我采样就是在比较中心的位置直接选取的，所以只会出现在一些窄带上的混杂现象，而不会出现如下这种较明显的无意义数字：
![[gen_img_4.jpg]]
![[notbad.png]]

---
### 5. 加入正则化

加入`L2`正则化，牵制一下特征分布并"铺开"latent_code。

```python
class Trainer:
    def __init__(self):
        """
        initialization...
        """
        self.opt = torch.optim.Adam(self.net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    def train(self):
        """
        set dict...
        constants setting...
        """
        for epochs in range(epoch):
            """
            variations...
            """
            for i, (x, y) in enumerate(Train_DataLoader):
                """
                add noise...
                process...
                """
                L2_Reg = 0.0
                for param in self.net.parameters():
                    L2_Reg += torch.norm(param, 2)
                out_img = self.net(img)
                loss = self.loss_fn(out_img, img) + WEIGHT_DECAY * L2_Reg
                """
                backprop...
                get latend code...
                print states...
                """
            """
            Save the reconstructed images...
            """
        """
        Save the model...
        """

    def generate(self):
        """
        Select an area of the latent space...
        Generate images from the selected area...
        save images...
        """

if __name__ == '__main__':
    t = Trainer()
    t.train()
    t.generate()
```

效果：
可以看到相较与未正则化之前，浅色部分的类别要更分散一些了，而不是聚集在同一个区域，我猜测正则化之后的图像分布，有圆圈的数字(如0, 6, 8, 9)效果会更加分明。
![[Latend_Codes 2.png]]

从(-5, -5)到(5, 5)均匀采样恢复数字，不小心把分界线采进去了，斜对角线上的数字意义非常不明确，但在分界线的两侧，能非常明显的看到数字的特征变化，总体来说效果还是很不错的。
![[generated_images.png]]

---
### # 探索(代码早期的困难与对照试验)

**bottleneck不同导致的生成效果不同：**
1. bottleneck = 2
![[gen_img__10.png]]
2. bottleneck = 128
![[gen_img_10.jpg]]

**损失函数不同导致的生成效果不同：**

_Loss function for binay inputs:_
$l(f(\mathbf{x}))=-\sum_k\left(x_k\log(\widehat{x}_k)+(1-x_k)\log(1-\widehat{x}_k)\right)$

_Loss function for real-valued inputs:_
$l(f(\mathbf{x}))=\frac{1}{2}\sum_k(\widehat{x}_k-x_k)^2$

1. MSE
![[gen_img_9.jpg]]
2. BCE
![[gen_img_10.jpg]]

**激活函数：**
1. ReLU后，做BCE之前过一个Sigmoid
![[LC1.png]]
2. 全部使用Sigmoid
![[Latend_Codes 1.png]]

**L2正则化：**
加了weight_decay之后Loss就恢复正常了🤓
_使用MSE时加入L2正则化效果正常(图一为正则化前，图二为正则化后)：_
![[MSE_NoL2.png]]
![[MSE_L2.png]]

**网络结构不同：**
CNN：
最开始写的四层CNN在bottleneck从128降至2之后变得稍有些模糊，在加入噪声并采用L2正则化之后性能非常差劲，可能是多层结构稍有些冗杂？后来修改的一版只有两层，效果有很好的提升。

MLP：
效果比CNN要好一些

---
## 实验二： All_Dogs AE
### 实验要求：

- 以MSE作为损失函数，设置$c$的维度为 8 × 8 × 16，搭建并训练深度自编码器网络。
- 随机选取9 张图片，分别展示每一张图片的原图和重建图像，并对latent code进行可视化。
- 随机选取256张图片，通过所构造的自编码器网络中的encoder得到其对应的latent code。计算这些 latent code的统计特性，并以此为参数构造高斯分布。试在你所得到的高斯分布上进行9 次随机采样，再将采样得到的9组latent code送入 decoder，观察所得到的图像并描述你观察到的现象。
- 在任务(3)的基础上，在这9 张图片的latent code上叠加随机的高斯噪声扰动，观察叠加噪声后的latent code送入decoder生成的图像，并解释你观察到的现象。
- 如下图所示，请将latent code叠加零均值高斯噪声作为一类正则自编码器方法，由此带噪训练新的正则自编码器 (限制latent code 维度为 8 × 8 × 16)。需要注意的是，为了保证高斯噪声具有稳定的效果，还需要在**叠加噪声前对latent code进行功率归一化**。请在噪声方差分别为0.05，0.1，0.15 时，给出Dog 数据集上重建图像PSNR的平均值，需要并探究此时从latent space 采样是否有生成效果
---
### 前言&资料：

这次没搞`transform`然后图片`size`也比较友好，整小数据集就没得好大意义。同时简单的代码片段并没有贴出展示，完整代码可在附件`All_Dogs_Kaggle.ipynb`中查看，因为在kaggle上的补全和报错啥的都比较烂，所以代码可能有冗余部分，还请见谅。

Latent_Code的大小选择：[What is an appropriate size for a latent space of (variational) autoencoders and how it varies with the features of the images?](https://ai.stackexchange.com/questions/37272/what-is-an-appropriate-size-for-a-latent-space-of-variational-autoencoders-and)

_an easy way is to try with different values and look at the likelihood on the test-set log(p)-pick the lowest dimensionality that maximises it. This is a solution in tune with Deep Learning spirit :)
Second solution, maybe a little more grounded, is to decompose your training data with SVD and look at the spectrum of singular values. The number of non trivial (=above some small threshold) values will give you a rough idea of the number of latent dimensions you are going to need._

解决重建模糊问题(但是VAE)：[Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder | OpenReview](https://openreview.net/forum?id=rkglvsC9Ym)

---
### 1. 网络搭建

```text
AutoEncoder(
  (encoder): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU()
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))
    (12): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
  )
  (decoder): Sequential(
    (0): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Upsample(scale_factor=2.0, mode='nearest')
    (4): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Upsample(scale_factor=2.0, mode='nearest')
    (8): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Upsample(scale_factor=2.0, mode='nearest')
    (12): Conv2d(48, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): Tanh()
  )
)
```

1. **池化层：** 最开始在未使用池化层的时候，因为多层卷积层叠加，会出现颜色特征消失的现象，图像变成类似灰度风格的图片，可能是未处理好网络结构导致在进行通道变换的时候出现了特征丢失的情况。
2. **池化后卷积：** 最后一层为池化层，在图像恢复后会出现网格状的块效应现象，能看到边缘泛出RGB的色晕，在这一层池化后加入`stride = 1`的卷积层后这种现象消失。好像还可以对图像扩充一些像素然后对扩充后的图像进行图像重建，图像块重建后对于重叠部分进行切除舍弃处理。
3. **上采样：** 其实`decoder`中复原图像使用`upsamlpe`效果要更好，因为利用`Maxpool2d`返回的idx可以更好的复原图像，但是因为后面要从`latent_code`再生成图像，所以就不咋能用了。

---
### 2. 抽取图片后重建

随机生成9个随机数后抽取图片，经过`encoder`后展示`latent_code`，再经`decoder`进行重建。
最开始手算的网络图像维度，最后发现`latent_code`好像大了一号，才知道把隐藏层维度弄成16\*16\*16了，按要求来展示的话确实要模糊一些。

![[Contract.png]]

---
### 3. 计算参数构造高斯分布

![[3e64cbe43ad05f8bdc5b8dcc280ec4f.png]]
VAE青春版，手动算一个均值和方差出来观察，把`Decoder`的输出改成高斯分布的均值和方差再放回去对比就可以升级为VAE了。

输出结果：
![[Pasted image 20231203120425.png]]

对比给出的分布图：
![[Pasted image 20231203120452.png]]
好像我这个偏差会更大一些，重心不在均值处，隐空间的分布情况比较有随机性。

#### 3.1 随机采样生成的9张图片

![[Pasted image 20231203121825.png]]
![[Pasted image 20231203121851.png]]
![[Pasted image 20231203121903.png]]

可以明显地观察出基本上都是无意义的生成图像，就像MNIST中直接对隐藏层空间的一片区域采样生成一样，直接采样很有可能采空，在隐空间随机一处无意义的地点生成的特征。
为了使生成的图像更真实一些，我向采样点增加了一个权重，去牵制它生成的位置，利用权重把采样点向真实存在的数据分布上拉扯。

#### 3.2 随机采样点牵制

~~最好的写法应该是在采样点周围找一个离它最近的真实样本点去拟合~~

```python
"""
We can tell from the Generated images that they don't look like dogs.
So we need to add some constraints to the latent code.
Let's make the latent code a bit closer to the real sampled dots.
"""

Delta = 0.25
Generated = []
Generated = np.random.multivariate_normal(Mean, Cov, 9)
Generated = Delta * Generated + (1 - Delta) * encoded_img[0:9]
Generated = Generated.reshape((-1, 16, 8, 8))
print(Generated.shape)
```

![[Pasted image 20231203122510.png]]
![[Pasted image 20231203122516.png]]
![[Pasted image 20231203122521.png]]

这样看上去就舒服多了，爱狗人士🐶有福了，辨认难度大大降低。
但其实我感觉这样对采样点进行加权，相比随机采样来说，更像是对图像进行加噪处理，轻微偏离采样点而已。
重新写一个仅加噪的版本，发现确实是这种感觉，不过加噪要更不自然一些，所以说加权也还算合理？至少看上去更接近真实图片。

```python
# Add Noise to the Generated
Factor = 0.25
Generated = []
Generated = np.random.multivariate_normal(Mean, Cov, 9)
Generated = encoded_img[0:9]
Generated = Generated + Factor * np.random.normal(0, 1, (9, 1024))
Generated = Generated.reshape((-1, 16, 8, 8))
print(Generated.shape)
```

![[Pasted image 20231203123332.png]]
![[Pasted image 20231203123341.png]]
![[Pasted image 20231203123347.png]]

---
### 4. 叠加噪声后再decoder
#### 4.1 对原始随机采样图片加噪
![[Pasted image 20231203125125.png]]
![[Pasted image 20231203125829.png]]
![[Pasted image 20231203125838.png]]
#### 4.2 对加权随机采样图片加噪
![[Pasted image 20231203130243.png]]
![[Pasted image 20231203130322.png]]
![[Pasted image 20231203130327.png]]

加入噪声之后大致的形状特征都保留下来了，色彩特征会收到一定的影响，但整体影响不大。

---
### 5. 加噪重建性能

#### 5.1 Standard

随便对一张图加噪后计算PSNR，观察图片区别与分贝数，有个大概的衡量标准。下图给出的示例基本上一模一样了，但只能达到46dB，所以说50多分贝确实是有点扯拐了。

```python
idx = np.random.randint(0, len(train_set), 1)
img = Train_Loader.dataset[idx[0]]
img = img.unsqueeze(0).to(device)
print(img.shape)
noise_factor = 0.05
img_slightly_noisy = img + noise_factor * torch.normal(mean=0.0, std=0.1, size=img.size()).to(device)
psnr = CalcuPSNR(img, img_slightly_noisy)
print(psnr)

# show 2 images
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(img.cpu().data.squeeze().permute(1, 2, 0))
plt.axis('off')
plt.subplot(1, 2, 2)
plt.imshow(img_slightly_noisy.cpu().data.squeeze().permute(1, 2, 0))
plt.axis('off')
plt.show()
```
```output
torch.Size([1, 3, 64, 64])
[46.07505505]
```
![[Pasted image 20231203132601.png]]
#### 5.2 实操

```python
# Noise
class add_noise(nn.Module):
    def __init__(self, std):
        super(add_noise, self).__init__()
        self.std = std  # std

    # generate Gaussian noise
    def gaussian_noise_layer(self, input_layer, std, noise_factor = 1):
        noise = torch.normal(mean=0.0, std=std, size=input_layer.size()) * noise_factor
        output = input_layer.to(device) + noise.to(device)
        return output

    # Normalize the latent code
    def normalize(self, x):
        pwr = torch.mean(x ** 2)
        out = x / torch.sqrt(pwr)
        return out

    def forward(self, input):
        latent_code = self.normalize(input)
        noisy_code = self.gaussian_noise_layer(latent_code, self.std)
        noisy_code = noisy_code.to(device)
        return noisy_code

# Train
class RealPuppyTrainer:
    def __init__(self, model):
        self.model = model
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
        self.criterion = nn.MSELoss()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.add_noise = add_noise(std=0.05) # std = 0.05/0.1/0.15
```

为了反映到底是加噪处理的性能效果不好还是AE本身生成能力的不足，我把两个PSNR都算出来了。
AVG_PSNR是对比原图和加噪生成图得到的
AVG_noisyPSNR是对比生成图和加噪生成图得到的

**std = 0.05:** 
```python:
Epoch: 22 | Total loss: 0.369172 | AVG_PSNR: 25.964dB | AVG_noisyPSNR: 42.247dB | Time: 28.19s
Epoch: 23 | Total loss: 0.370368 | AVG_PSNR: 25.727dB | AVG_noisyPSNR: 41.965dB | Time: 27.55s
Epoch: 24 | Total loss: 0.376545 | AVG_PSNR: 25.733dB | AVG_noisyPSNR: 42.165dB | Time: 27.66s
Epoch: 25 | Total loss: 0.357900 | AVG_PSNR: 26.161dB | AVG_noisyPSNR: 41.771dB | Time: 27.71s
Epoch: 26 | Total loss: 0.356455 | AVG_PSNR: 25.992dB | AVG_noisyPSNR: 41.732dB | Time: 28.10s
```
![[Denoising_Epoch_5.png]]
**std = 0.1:** 
```python:
Epoch: 17 | Total loss: 0.372735 | AVG_PSNR: 23.772dB | AVG_noisyPSNR: 37.195dB | Time: 26.22s
Epoch: 18 | Total loss: 0.373087 | AVG_PSNR: 24.039dB | AVG_noisyPSNR: 37.041dB | Time: 26.61s
Epoch: 19 | Total loss: 0.350900 | AVG_PSNR: 24.386dB | AVG_noisyPSNR: 37.146dB | Time: 26.78s
Epoch: 20 | Total loss: 0.368970 | AVG_PSNR: 24.615dB | AVG_noisyPSNR: 36.966dB | Time: 26.55s
Epoch: 21 | Total loss: 0.353576 | AVG_PSNR: 24.865dB | AVG_noisyPSNR: 36.973dB | Time: 26.59s
```
![[Denoising_Epoch_22 (1).png]]
**std = 0.15:** 
```python:
Epoch: 19 | Total loss: 0.368680 | AVG_PSNR: 23.751dB | AVG_noisyPSNR: 35.735dB | Time: 26.10s
Epoch: 20 | Total loss: 0.366096 | AVG_PSNR: 24.094dB | AVG_noisyPSNR: 36.174dB | Time: 27.14s
Epoch: 21 | Total loss: 0.367677 | AVG_PSNR: 24.113dB | AVG_noisyPSNR: 36.138dB | Time: 26.37s
Epoch: 22 | Total loss: 0.363055 | AVG_PSNR: 24.594dB | AVG_noisyPSNR: 35.729dB | Time: 27.35s
Epoch: 23 | Total loss: 0.356284 | AVG_PSNR: 24.462dB | AVG_noisyPSNR: 35.908dB | Time: 26.28s
```
![[Denoising_Epoch_21.png]]

观察可以得出，随着`std`的增加，`PSNR`逐渐减小，主要体现在重构图像与加噪重构图像上的差别，而原图和加噪重构图像上区别不算特别大。
性能其实还算不错了，基本上是是完美重构了，再想要提升只能修改网络结构或者修改bottleneck大小，只能说隐空间确实有点随机。

**Cheat版本，16\*16\*16的latent_code：**
重建出来之后看上去已经非常相似了，所以这样一看好像25左右的PSNR已经非常优秀了，也没必要去嗯卷准确率了。
![[Denoising_Epoch_29.png]]
```text
...
Epoch: 26 | Total loss: 0.075562 | AVG_PSNR: 31.904dB | Time: 25.48s
Epoch: 27 | Total loss: 0.076207 | AVG_PSNR: 31.163dB | Time: 25.50s
Epoch: 28 | Total loss: 0.071224 | AVG_PSNR: 31.296dB | Time: 26.06s
Epoch: 29 | Total loss: 0.070521 | AVG_PSNR: 32.058dB | Time: 26.14s
Epoch: 30 | Total loss: 0.066973 | AVG_PSNR: 30.472dB | Time: 25.45s
...
```

---
### # 小剧场：

![[e9fe2dd1d3bd46395e98036c26977bd.png]]
